{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def my_confusion_matrix(array_Expected,array_Predicted,colName):\n",
    "    a = np.array(confusion_matrix(array_Expected, array_Predicted ))\n",
    "    totalExpectedFalse = a[0,0] + a[0,1]\n",
    "    totalExpectedTrue = a[1,0] + a[1,1]\n",
    "    correctFalse = a[0,0] \n",
    "    correctTrue = a[1,1] \n",
    "    correctTruePct = np.round(correctTrue / totalExpectedTrue,3)\n",
    "    correctFalsePct = np.round(correctFalse / totalExpectedFalse,3)\n",
    "    print('Regarding ' + colName + '...')\n",
    "    print('The model correctly predicted {} Austens out of {} expected Austens: {}'.format(\n",
    "        correctFalse,totalExpectedFalse,correctFalsePct))\n",
    "    print('The model correctly predicted {} Carrols out of {} expected Carrols: {}'.format(\n",
    "        correctTrue,totalExpectedTrue,correctTruePct))    \n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 ms, sys: 10.5 ms, total: 42 ms\n",
      "Wall time: 96.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "emma = re.sub(r'CHAPTER .*', '', emma)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "emma = text_cleaner(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "CPU times: user 59.4 s, sys: 22.3 s, total: 1min 21s\n",
      "Wall time: 54.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "emma_doc = nlp(emma)\n",
    "\n",
    "print(type(alice_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\", \"Alice in Wonderland\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\", 'Persuasion'] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\", 'Emma'] for sent in emma_doc.sents]\n",
    "\n",
    "# Combine the sentences from the 3 novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents[0:1000] + persuasion_sents[0:1000] + emma_sents[0:1000])\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her. There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge. In another moment down went Alice after it, never once considering how in the world she was to get out again. The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well. Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it. 'Well!' thought Alice to herself, 'after such a fall as this, I shall think nothing of tumbling down stairs! How brave they'll all think me at home! Why, I wouldn't say anything about it, even if I fell off the top of the house!' (Which was very likely true.) Down, down, down. Would the fall NEVER come to an end! 'I wonder how many miles I've fallen by this time?' she said aloud. 'I must be getting somewhere near the centre of the earth. Let me see: that would be four thousand miles down, I think ' (for, you see, Alice had learnt several things of this sort in her lessons in the schoolroom, and though this was not a VERY good opportunity for showing off her knowledge, as there was no one to listen to her, still it was good practice to say it over) ' yes, that's about the right distance but then I wonder what Latitude or Longitude I've got to?' (Alice had no idea what Latitude was, or Longitude either, but thought they were nice grand words to say.) Presently she began again. 'I wonder if I shall fall right THROUGH the earth! How funny it'll seem to come out among the people that walk with their heads downward! The Antipathies, I think ' (she was rather glad there WAS no one listening, this time, as it didn't sound at all the right word) ' but I shall have to ask them what the name of the country is, you know. Please, Ma'am, is this New Zealand or Australia?' (and she tried to curtsey as she spoke fancy CURTSEYING as you're falling through the air! Do you think you could manage it?) 'And what an ignorant little girl she'll think me for asking! No, it'll never do to ask: perhaps I shall see it written up somewhere.' Down, down, down. There was nothing else to do, so Alice soon began talking again. 'Dinah'll miss me very much to-night, I should think!' (Dinah was the cat.) 'I hope they'll remember her saucer of milk at tea-time. Dinah my dear! I wish you were down here with me! There are no mice in the air, I'm afraid, but you might catch a bat,\n"
     ]
    }
   ],
   "source": [
    "print(alice_doc[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
      "her\n",
      "So  :  RB\n",
      "she  :  PRP\n",
      "was  :  VBD\n",
      "considering  :  VBG\n",
      "in  :  IN\n",
      "her  :  PRP$\n",
      "own  :  JJ\n",
      "mind  :  NN\n",
      "(  :  -LRB-\n",
      "as  :  RB\n",
      "well  :  RB\n",
      "as  :  IN\n",
      "she  :  PRP\n",
      "could  :  MD\n",
      ",  :  ,\n",
      "for  :  IN\n",
      "the  :  DT\n",
      "hot  :  JJ\n",
      "day  :  NN\n",
      "made  :  VBD\n",
      "her  :  PRP\n",
      "feel  :  VB\n",
      "very  :  RB\n",
      "sleepy  :  JJ\n",
      "and  :  CC\n",
      "stupid  :  JJ\n",
      ")  :  -RRB-\n",
      ",  :  ,\n",
      "whether  :  IN\n",
      "the  :  DT\n",
      "pleasure  :  NN\n",
      "of  :  IN\n",
      "making  :  VBG\n",
      "a  :  DT\n",
      "daisy  :  NN\n",
      "-  :  HYPH\n",
      "chain  :  NN\n",
      "would  :  MD\n",
      "be  :  VB\n",
      "worth  :  JJ\n",
      "the  :  DT\n",
      "trouble  :  NN\n",
      "of  :  IN\n",
      "getting  :  VBG\n",
      "up  :  RP\n",
      "and  :  CC\n",
      "picking  :  VBG\n",
      "the  :  DT\n",
      "daisies  :  NNS\n",
      ",  :  ,\n",
      "when  :  WRB\n",
      "suddenly  :  RB\n",
      "a  :  DT\n",
      "White  :  NNP\n",
      "Rabbit  :  NNP\n",
      "with  :  IN\n",
      "pink  :  JJ\n",
      "eyes  :  NNS\n",
      "ran  :  VBD\n",
      "close  :  RB\n",
      "by  :  IN\n",
      "her  :  PRP\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences.loc[1:1,0]:\n",
    "    print(sentence)\n",
    "    print(sentence[-2])\n",
    "        \n",
    "#token.pos_\n",
    "    for token in sentence:\n",
    "         print('{}  :  {}'.format(token,token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function to create a list of the most common words in a given document.\n",
    "def bag_of_words(text, n, minlen=0, maxlen=99):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct    #disregard punctuation\n",
    "                and not token.is_stop    #disregard stop words\n",
    "                and not token.pos_ == 'PROPN'   #disregard proper nouns\n",
    "                and len(token) >= minlen   #only get those with minimum length\n",
    "                and len(token) < maxlen]   \n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item for item in Counter(allwords).most_common(n)]\n",
    "    \n",
    "\n",
    "# Set up a bag of words of the most common small words in the 3 novels\n",
    "alicewords = bag_of_words(alice_doc, 200, 0, 7)\n",
    "persuasionwords = bag_of_words(persuasion_doc, 200, 0, 7)\n",
    "emmawords = bag_of_words(emma_doc, 200, 0, 7)\n",
    "\n",
    "# Combine bags to create a set of unique small words.\n",
    "all_words = (alicewords + persuasionwords + emmawords)\n",
    "df = pd.DataFrame(all_words)\n",
    "df = df.loc[df[0].isin(['-PRON-',\"'s\"]) ==False]\n",
    "df = df.groupby([0]).sum().sort_values(by=1, ascending=False)\n",
    "frequent_small_words = list(df[0:50].index)\n",
    "\n",
    "\n",
    "# Set up a bag of words of the most common big words in the 3 novels\n",
    "alicewords = bag_of_words(alice_doc, n=200, minlen=7)\n",
    "persuasionwords = bag_of_words(persuasion_doc, n=200, minlen=7)\n",
    "emmawords = bag_of_words(emma_doc, n=200, minlen=7)\n",
    "\n",
    "# Combine bags to create a set of unique big words.\n",
    "all_words = (alicewords + persuasionwords + emmawords)\n",
    "df = pd.DataFrame(all_words)\n",
    "df = df.loc[df[0].isin(['-PRON-',\"'s\"]) ==False]\n",
    "df = df.groupby([0]).sum().sort_values(by=1, ascending=False)\n",
    "frequent_big_words = list(df[0:50].index)\n",
    "\n",
    "\n",
    "frequent_words = frequent_small_words + frequent_big_words\n",
    "len(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 100\n",
      "Processing 200\n",
      "Processing 300\n",
      "Processing 400\n",
      "Processing 500\n",
      "Processing 600\n",
      "Processing 700\n",
      "Processing 800\n",
      "Processing 900\n",
      "Processing 1000\n",
      "Processing 1100\n",
      "Processing 1200\n",
      "Processing 1300\n",
      "Processing 1400\n",
      "Processing 1500\n",
      "Processing 1600\n",
      "Processing 1700\n",
      "Processing 1800\n",
      "Processing 1900\n",
      "Processing 2000\n",
      "Processing 2100\n",
      "Processing 2200\n",
      "Processing 2300\n",
      "Processing 2400\n",
      "Processing 2500\n",
      "Processing 2600\n",
      "Processing 2700\n",
      "Processing 2800\n",
      "Processing 2900\n",
      "Filling NAs with 0\n",
      "Done\n",
      "(3000, 109)\n"
     ]
    }
   ],
   "source": [
    "# Creates a data frame with features for each word in our word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    \n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Processing ' + str(i))\n",
    "        \n",
    "        #features: populate sentence length\n",
    "        df.loc[i,'numwords'] = len(sentence)\n",
    "        #df.loc[i,'punct'] = sentence[-1]\n",
    "        df.loc[i, common_words] = 0\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, proper nouns, and words not in our master list.\n",
    "#         words = [token.lemma_\n",
    "#                  for token in sentence\n",
    "#                  if (\n",
    "#                      not token.is_stop\n",
    "#                      and not token.pos_ == 'PROPN'\n",
    "#                      and token.lemma_ in common_words\n",
    "#                  )]\n",
    "        \n",
    "#         punctuation = [token\n",
    "#                  for token in sentence\n",
    "#                  if (\n",
    "#                     token.is_punct == True\n",
    "#                  )]\n",
    "\n",
    "#         propernouns = [token\n",
    "#              for token in sentence\n",
    "#              if (\n",
    "#                 token.pos_ == 'PROPN'\n",
    "#              )]\n",
    "\n",
    "#         uppers = [token\n",
    "#              for token in sentence\n",
    "#              if (\n",
    "#                 token.is_upper == True\n",
    "#              )]\n",
    "        num_punct = 0\n",
    "        num_propn = 0\n",
    "        num_words = 0\n",
    "        num_char = 0\n",
    "        bool_upper = 0\n",
    "        num_repeat = 0\n",
    "        num_ing = 0\n",
    "        prior_token = ''\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token.is_punct == True:\n",
    "                num_punct = num_punct + 1\n",
    "            elif token.pos_ == 'PROPN':\n",
    "                num_propn = num_propn + 1\n",
    "            else:\n",
    "                num_words = num_words + 1\n",
    "                num_char = num_char + len(token)\n",
    "            \n",
    "            if token.is_upper == True:\n",
    "                bool_upper = 1\n",
    "               \n",
    "            if token.lemma_ in common_words:\n",
    "                df.loc[i, token.lemma_] += 1\n",
    "            \n",
    "            if token.norm_ == prior_token:\n",
    "                num_repeat = num_repeat +1\n",
    "            \n",
    "            if token.suffix_ == 'ing':\n",
    "                num_ing = num_ing + 1\n",
    "                \n",
    "            prior_token = token.norm_\n",
    "            \n",
    "        #feature: Populate the row with word counts.\n",
    "#         for word in words:\n",
    "#             df.loc[i, word] += 1\n",
    "            \n",
    "        \n",
    "        #feature:  Avg Word Size\n",
    "#         numchars = sentence.end_char - sentence.start_char\n",
    "#         avgwordsize = numchars / len(sentence)\n",
    "        if num_words > 0:\n",
    "            avgwordsize = num_char / num_words\n",
    "        else:\n",
    "            avgwordsize = 0\n",
    "        df.loc[i,'avgwordsize'] = avgwordsize\n",
    "        \n",
    "        #feature: Num Punct\n",
    "        df.loc[i,'numpunct'] = num_punct  #  len(punctuation)\n",
    "        \n",
    "        #feature: Num Proper Nouns\n",
    "        df.loc[i,'numpropernoun'] = num_propn   #len(propernouns)\n",
    "\n",
    "        #feature: Num Uppercase words\n",
    "        df.loc[i,'upperword'] = bool_upper   #np.where(len(uppers) > 0, 1, 0)\n",
    "        \n",
    "        #feature: repeats\n",
    "        df.loc[i,'numrepeat'] = num_repeat\n",
    "        \n",
    "        #feature: ending in ing\n",
    "        df.loc[i,'numing'] = num_ing\n",
    "        \n",
    "        #feature: first word part of speech\n",
    "        df.loc[i,'first_pos'] = sentence[0].pos_\n",
    "        \n",
    "        #feature: last word part of speech\n",
    "        df.loc[i,'last_pos'] = sentence[-2].pos_\n",
    "        \n",
    "    print('Filling NAs with 0')\n",
    "    df.fillna(0,inplace=True)\n",
    "    print('Done')\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = bow_features(sentences.loc[:,0], frequent_words)\n",
    "df[['author','title']] = sentences.loc[:,1:2] \n",
    "\n",
    "df_first_pos = pd.get_dummies(df.first_pos, prefix='first')\n",
    "df_last_pos = pd.get_dummies(df.last_pos, prefix='last')\n",
    "df = pd.concat([df, df_first_pos, df_last_pos], axis=1)\n",
    "df.drop(columns=['first_pos','last_pos'], inplace=True)\n",
    "\n",
    "\n",
    "#df[['begin','sit','have','maxrepeats']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.152333</td>\n",
       "      <td>0.372166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.294821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.072667</td>\n",
       "      <td>0.272172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.241281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>1.311112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.072667</td>\n",
       "      <td>0.275823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.302191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.220924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.228726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.278055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.170333</td>\n",
       "      <td>0.398379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.040333</td>\n",
       "      <td>0.206690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>0.219137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.217310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.249517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>1.172077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.167948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>0.164263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hear</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.024667</td>\n",
       "      <td>0.155133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soon</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>0.171063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.774905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.134208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.189871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.172088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dear</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.165327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.024667</td>\n",
       "      <td>0.157268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.196077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.146806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.023667</td>\n",
       "      <td>0.154211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpropernoun</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.652871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upperword</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.454299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numrepeat</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.065333</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numing</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.628333</td>\n",
       "      <td>1.057625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_ADJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.036333</td>\n",
       "      <td>0.187149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_ADP</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.222064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_ADV</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.107667</td>\n",
       "      <td>0.310011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_CCONJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.250064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_DET</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.092667</td>\n",
       "      <td>0.290013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_INTJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.038667</td>\n",
       "      <td>0.192831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_NOUN</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.173347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_NUM</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.090921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_PART</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_PRON</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.280333</td>\n",
       "      <td>0.449237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_PROPN</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.100333</td>\n",
       "      <td>0.300494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_PUNCT</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.124333</td>\n",
       "      <td>0.330016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_VERB</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.236908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_ADJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.236908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_ADP</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.108903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_ADV</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.252929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_CCONJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_DET</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.096173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_INTJ</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.114717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_NOUN</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.259333</td>\n",
       "      <td>0.438342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_NUM</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.070545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PART</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.104320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PRON</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.081667</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PROPN</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.056333</td>\n",
       "      <td>0.230603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PUNCT</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.465777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_VERB</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.306605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                count      mean       std  min  25%  50%  75%   max\n",
       "say            3000.0  0.152333  0.372166  0.0  0.0  0.0  0.0   2.0\n",
       "good           3000.0  0.076000  0.294821  0.0  0.0  0.0  0.0   4.0\n",
       "know           3000.0  0.072667  0.272172  0.0  0.0  0.0  0.0   2.0\n",
       "come           3000.0  0.056000  0.241281  0.0  0.0  0.0  0.0   2.0\n",
       "the            3000.0  0.835000  1.311112  0.0  0.0  0.0  1.0  12.0\n",
       "little         3000.0  0.072667  0.275823  0.0  0.0  0.0  0.0   2.0\n",
       "think          3000.0  0.093333  0.302191  0.0  0.0  0.0  0.0   2.0\n",
       "thing          3000.0  0.047000  0.220924  0.0  0.0  0.0  0.0   2.0\n",
       "time           3000.0  0.048667  0.228726  0.0  0.0  0.0  0.0   3.0\n",
       "go             3000.0  0.073333  0.278055  0.0  0.0  0.0  0.0   2.0\n",
       "but            3000.0  0.170333  0.398379  0.0  0.0  0.0  0.0   3.0\n",
       "man            3000.0  0.036667  0.200006  0.0  0.0  0.0  0.0   3.0\n",
       "look           3000.0  0.040333  0.206690  0.0  0.0  0.0  0.0   2.0\n",
       "great          3000.0  0.044667  0.219137  0.0  0.0  0.0  0.0   3.0\n",
       "like           3000.0  0.042333  0.217310  0.0  0.0  0.0  0.0   2.0\n",
       "see            3000.0  0.061333  0.249517  0.0  0.0  0.0  0.0   2.0\n",
       "be             3000.0  0.981000  1.172077  0.0  0.0  1.0  1.0   9.0\n",
       "feel           3000.0  0.028333  0.167948  0.0  0.0  0.0  0.0   2.0\n",
       "day            3000.0  0.026333  0.164263  0.0  0.0  0.0  0.0   2.0\n",
       "hear           3000.0  0.024667  0.155133  0.0  0.0  0.0  0.0   1.0\n",
       "soon           3000.0  0.027333  0.171063  0.0  0.0  0.0  0.0   3.0\n",
       "have           3000.0  0.416000  0.774905  0.0  0.0  0.0  1.0   5.0\n",
       "and            3000.0  0.700000  1.134208  0.0  0.0  0.0  1.0  11.0\n",
       "find           3000.0  0.031000  0.189871  0.0  0.0  0.0  0.0   2.0\n",
       "father         3000.0  0.027000  0.172088  0.0  0.0  0.0  0.0   2.0\n",
       "dear           3000.0  0.026000  0.165327  0.0  0.0  0.0  0.0   2.0\n",
       "wish           3000.0  0.024667  0.157268  0.0  0.0  0.0  0.0   2.0\n",
       "way            3000.0  0.030000  0.196077  0.0  0.0  0.0  0.0   5.0\n",
       "shall          3000.0  0.021333  0.146806  0.0  0.0  0.0  0.0   2.0\n",
       "long           3000.0  0.023667  0.154211  0.0  0.0  0.0  0.0   2.0\n",
       "...               ...       ...       ...  ...  ...  ...  ...   ...\n",
       "numpropernoun  3000.0  1.077000  1.652871  0.0  0.0  0.0  2.0  18.0\n",
       "upperword      3000.0  0.291000  0.454299  0.0  0.0  0.0  1.0   1.0\n",
       "numrepeat      3000.0  0.065333  0.542060  0.0  0.0  0.0  0.0  17.0\n",
       "numing         3000.0  0.628333  1.057625  0.0  0.0  0.0  1.0   9.0\n",
       "first_ADJ      3000.0  0.036333  0.187149  0.0  0.0  0.0  0.0   1.0\n",
       "first_ADP      3000.0  0.052000  0.222064  0.0  0.0  0.0  0.0   1.0\n",
       "first_ADV      3000.0  0.107667  0.310011  0.0  0.0  0.0  0.0   1.0\n",
       "first_CCONJ    3000.0  0.067000  0.250064  0.0  0.0  0.0  0.0   1.0\n",
       "first_DET      3000.0  0.092667  0.290013  0.0  0.0  0.0  0.0   1.0\n",
       "first_INTJ     3000.0  0.038667  0.192831  0.0  0.0  0.0  0.0   1.0\n",
       "first_NOUN     3000.0  0.031000  0.173347  0.0  0.0  0.0  0.0   1.0\n",
       "first_NUM      3000.0  0.008333  0.090921  0.0  0.0  0.0  0.0   1.0\n",
       "first_PART     3000.0  0.001667  0.040798  0.0  0.0  0.0  0.0   1.0\n",
       "first_PRON     3000.0  0.280333  0.449237  0.0  0.0  0.0  1.0   1.0\n",
       "first_PROPN    3000.0  0.100333  0.300494  0.0  0.0  0.0  0.0   1.0\n",
       "first_PUNCT    3000.0  0.124333  0.330016  0.0  0.0  0.0  0.0   1.0\n",
       "first_VERB     3000.0  0.059667  0.236908  0.0  0.0  0.0  0.0   1.0\n",
       "last_ADJ       3000.0  0.059667  0.236908  0.0  0.0  0.0  0.0   1.0\n",
       "last_ADP       3000.0  0.012000  0.108903  0.0  0.0  0.0  0.0   1.0\n",
       "last_ADV       3000.0  0.068667  0.252929  0.0  0.0  0.0  0.0   1.0\n",
       "last_CCONJ     3000.0  0.000667  0.025816  0.0  0.0  0.0  0.0   1.0\n",
       "last_DET       3000.0  0.009333  0.096173  0.0  0.0  0.0  0.0   1.0\n",
       "last_INTJ      3000.0  0.013333  0.114717  0.0  0.0  0.0  0.0   1.0\n",
       "last_NOUN      3000.0  0.259333  0.438342  0.0  0.0  0.0  1.0   1.0\n",
       "last_NUM       3000.0  0.005000  0.070545  0.0  0.0  0.0  0.0   1.0\n",
       "last_PART      3000.0  0.011000  0.104320  0.0  0.0  0.0  0.0   1.0\n",
       "last_PRON      3000.0  0.081667  0.273902  0.0  0.0  0.0  0.0   1.0\n",
       "last_PROPN     3000.0  0.056333  0.230603  0.0  0.0  0.0  0.0   1.0\n",
       "last_PUNCT     3000.0  0.318000  0.465777  0.0  0.0  0.0  1.0   1.0\n",
       "last_VERB      3000.0  0.105000  0.306605  0.0  0.0  0.0  0.0   1.0\n",
       "\n",
       "[133 rows x 8 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'first_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3476ca9359d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_pos\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'first_pos'"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab( df.first_pos , df.author,  margins=True, normalize='columns' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avgwordsize</th>\n",
       "      <th>maxrepeats</th>\n",
       "      <th>numpropernoun</th>\n",
       "      <th>numpunct</th>\n",
       "      <th>numuppers</th>\n",
       "      <th>numwords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Austen</th>\n",
       "      <th>Emma</th>\n",
       "      <td>4.261418</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>0.940667</td>\n",
       "      <td>3.249333</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>21.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persuasion</th>\n",
       "      <td>4.502711</td>\n",
       "      <td>0.908667</td>\n",
       "      <td>1.499333</td>\n",
       "      <td>4.403333</td>\n",
       "      <td>0.301333</td>\n",
       "      <td>30.900667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <th>Alice in Wonderland</th>\n",
       "      <td>3.871423</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>4.242667</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>20.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <th></th>\n",
       "      <td>4.211851</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>3.965111</td>\n",
       "      <td>0.384889</td>\n",
       "      <td>24.339333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             avgwordsize  maxrepeats  numpropernoun  numpunct  \\\n",
       "author  title                                                                   \n",
       "Austen  Emma                    4.261418    0.854667       0.940667  3.249333   \n",
       "        Persuasion              4.502711    0.908667       1.499333  4.403333   \n",
       "Carroll Alice in Wonderland     3.871423    0.936667       0.818000  4.242667   \n",
       "All                             4.211851    0.900000       1.086000  3.965111   \n",
       "\n",
       "                             numuppers   numwords  \n",
       "author  title                                      \n",
       "Austen  Emma                  0.349333  21.573333  \n",
       "        Persuasion            0.301333  30.900667  \n",
       "Carroll Alice in Wonderland   0.504000  20.544000  \n",
       "All                           0.384889  24.339333  "
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pivot_table(index=['author','title'], values=['numwords','maxrepeats','avgwordsize','numpunct',\n",
    "                                                 'numpropernoun','numuppers'], \n",
    "                                               aggfunc=np.mean, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 133)\n",
      "(2400,)\n",
      "(400, 133)\n",
      "(400,)\n",
      "(400, 133)\n",
      "(400,)\n",
      "(600, 133)\n",
      "(600,)\n",
      "(3000, 133)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "#Train on 2 books\n",
    "#- Alice by Carrol\n",
    "#- Persuasion by Austin\n",
    "\n",
    "X_Alice = df.loc[df.title.isin(['Alice in Wonderland'])]\n",
    "Y_Alice = X_Alice.author\n",
    "X_Alice = X_Alice.drop(columns=['author','title'])\n",
    "\n",
    "X_Persuasion = df.loc[df.title.isin(['Persuasion'])]\n",
    "Y_Persuasion = X_Persuasion.author\n",
    "X_Persuasion = X_Persuasion.drop(columns=['author','title'])\n",
    "\n",
    "X_Emma = df.loc[df.title.isin(['Emma'])]\n",
    "Y_Emma = X_Emma.author\n",
    "X_Emma = X_Emma.drop(columns=['author','title'])\n",
    "\n",
    "\n",
    "#Train on first 1000 records from Alice and Persuasion\n",
    "X_Train = pd.concat([X_Alice[0:800], X_Persuasion[0:800], X_Emma[0:800]], axis=0)   #X_Emma[0:1000]], axis=0)\n",
    "Y_Train = pd.concat([Y_Alice[0:800], Y_Persuasion[0:800], Y_Emma[0:800]], axis=0)    #Y_Emma[0:1000]], axis=0)\n",
    "\n",
    "#Test1 on last 300 records from Alice and Persuasion\n",
    "X_Test1 = pd.concat([X_Alice[800:1000], X_Persuasion[800:1000]], axis=0)\n",
    "Y_Test1 = pd.concat([Y_Alice[800:1000], Y_Persuasion[800:1000]], axis=0)\n",
    "\n",
    "#Test2 on last 300 records from Alice and Emma\n",
    "X_Test2 = pd.concat([X_Alice[800:1000], X_Emma[800:1000]], axis=0)\n",
    "Y_Test2 = pd.concat([Y_Alice[800:1000], Y_Emma[800:1000]], axis=0)\n",
    "\n",
    "#Test3 combines last 300 from all 3 books\n",
    "X_Test3 = pd.concat([X_Alice[800:1000], X_Emma[800:1000], X_Persuasion[800:1000]], axis=0)\n",
    "Y_Test3 = pd.concat([Y_Alice[800:1000], Y_Emma[800:1000], Y_Persuasion[800:1000]], axis=0)\n",
    "\n",
    "\n",
    "#Full Set for Cross Validation\n",
    "X_Full = pd.concat([X_Alice, X_Persuasion, X_Emma], axis=0)\n",
    "Y_Full = pd.concat([Y_Alice, Y_Persuasion, Y_Emma], axis=0)\n",
    "\n",
    "print(X_Train.shape)\n",
    "print(Y_Train.shape)\n",
    "print(X_Test1.shape)\n",
    "print(Y_Test1.shape)\n",
    "print(X_Test2.shape)\n",
    "print(Y_Test2.shape)\n",
    "print(X_Test3.shape)\n",
    "print(Y_Test3.shape)\n",
    "print(X_Full.shape)\n",
    "print(Y_Full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: Training set score (Alice/Persuasion): 0.9945833333333334\n",
      "\n",
      "RF: Test set score (Alice/Persuasion): 0.8175\n",
      "\n",
      "RF: Test set score (Alice/Emma): 0.82\n",
      "\n",
      "RF: Test set score (Alice/Emma/Persuasion): 0.8416666666666667\n",
      "\n",
      "Regarding Austen...\n",
      "The model correctly predicted 355 Austens out of 400 expected Austens: 0.888\n",
      "The model correctly predicted 150 Carrols out of 200 expected Carrols: 0.75\n",
      "[[355  45]\n",
      " [ 50 150]]\n",
      "\n",
      "RF: Cross Validation (All Records) Accuracy 5 folds: 0.80 (+/- 0.04)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avgwordsize</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numwords</th>\n",
       "      <td>68.387872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PUNCT</th>\n",
       "      <td>64.180153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpropernoun</th>\n",
       "      <td>52.092163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpunct</th>\n",
       "      <td>44.672972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>34.960283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>31.299548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>29.318123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>27.832392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numrepeat</th>\n",
       "      <td>20.517775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Importance\n",
       "avgwordsize    100.000000\n",
       "numwords        68.387872\n",
       "last_PUNCT      64.180153\n",
       "numpropernoun   52.092163\n",
       "numpunct        44.672972\n",
       "say             34.960283\n",
       "be              31.299548\n",
       "the             29.318123\n",
       "have            27.832392\n",
       "numrepeat       20.517775"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=60, max_depth=30)\n",
    "\n",
    "train = rfc.fit(X_Train, Y_Train)\n",
    "\n",
    "print('RF: Training set score (Alice/Persuasion):', rfc.score(X_Train, Y_Train))\n",
    "print('\\nRF: Test set score (Alice/Persuasion):', rfc.score(X_Test1, Y_Test1))\n",
    "print('\\nRF: Test set score (Alice/Emma):', rfc.score(X_Test2, Y_Test2))\n",
    "\n",
    "print('\\nRF: Test set score (Alice/Emma/Persuasion):', rfc.score(X_Test3, Y_Test3))\n",
    "print('')\n",
    "Y_Pred = rfc.predict(X_Test3)\n",
    "my_confusion_matrix(Y_Test3, Y_Pred, 'Austen')\n",
    "\n",
    "score = cross_val_score(rfc, X_Full, Y_Full, cv=5)\n",
    "print(\"\\nRF: Cross Validation (All Records) Accuracy %i folds: %.2f (+/- %.2f)\" % (5, score.mean(), (score.std() * 2)))\n",
    "\n",
    "\n",
    "\n",
    "feature_importance = rfc.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "fi = pd.DataFrame(feature_importance, index=X_Train.columns, columns=['Importance'])\n",
    "fi.sort_values(by='Importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: Training set score (Alice/Persuasion): 0.84625\n",
      "\n",
      "LR: Test set score (Alice/Persuasion): 0.805\n",
      "\n",
      "LR: Test set score (Alice/Emma): 0.79\n",
      "\n",
      "LR: Test set score (Alice/Emma/Persuasion): 0.815\n",
      "\n",
      "Regarding Austen...\n",
      "The model correctly predicted 340 Austens out of 400 expected Austens: 0.85\n",
      "The model correctly predicted 149 Carrols out of 200 expected Carrols: 0.745\n",
      "[[340  60]\n",
      " [ 51 149]]\n",
      "\n",
      "LR: Cross Validation (All Books) Accuracy 5 folds: 0.81 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=100)\n",
    "train = lr.fit(X_Train, Y_Train)\n",
    "\n",
    "print('LR: Training set score (Alice/Persuasion):', lr.score(X_Train, Y_Train))\n",
    "print('\\nLR: Test set score (Alice/Persuasion):', lr.score(X_Test1, Y_Test1))\n",
    "print('\\nLR: Test set score (Alice/Emma):', lr.score(X_Test2, Y_Test2))\n",
    "\n",
    "print('\\nLR: Test set score (Alice/Emma/Persuasion):', lr.score(X_Test3, Y_Test3))\n",
    "print('')\n",
    "Y_Pred = lr.predict(X_Test3)\n",
    "my_confusion_matrix(Y_Test3, Y_Pred, 'Austen')\n",
    "\n",
    "\n",
    "score = cross_val_score(lr, X_Full, Y_Full, cv=5)\n",
    "print(\"\\nLR: Cross Validation (All Books) Accuracy %i folds: %.2f (+/- %.2f)\" % (5, score.mean(), (score.std() * 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFGB: Training set score (Alice/Persuasion): 0.8995833333333333\n",
      "\n",
      "RFGB: Test set score (Alice/Persuasion): 0.845\n",
      "\n",
      "RFGB: Test set score (Alice/Emma): 0.8325\n",
      "\n",
      "RFGB: Test set score (Alice/Emma/Persuasion): 0.8533333333333334\n",
      "\n",
      "Regarding Austen...\n",
      "The model correctly predicted 353 Austens out of 400 expected Austens: 0.882\n",
      "The model correctly predicted 159 Carrols out of 200 expected Carrols: 0.795\n",
      "[[353  47]\n",
      " [ 41 159]]\n",
      "\n",
      "RFGBCross Validation (All Books) Accuracy 5 folds: 0.81 (+/- 0.05)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avgwordsize</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numwords</th>\n",
       "      <td>65.472022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpropernoun</th>\n",
       "      <td>52.357778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_PUNCT</th>\n",
       "      <td>41.757733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpunct</th>\n",
       "      <td>41.191577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>30.493957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>27.274533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upperword</th>\n",
       "      <td>22.872968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>21.249588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>20.418504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Importance\n",
       "avgwordsize    100.000000\n",
       "numwords        65.472022\n",
       "numpropernoun   52.357778\n",
       "last_PUNCT      41.757733\n",
       "numpunct        41.191577\n",
       "the             30.493957\n",
       "be              27.274533\n",
       "upperword       22.872968\n",
       "have            21.249588\n",
       "say             20.418504"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier(learning_rate=.1, n_estimators=200)\n",
    "train = clf.fit(X_Train, Y_Train)\n",
    "\n",
    "print('RFGB: Training set score (Alice/Persuasion):', clf.score(X_Train, Y_Train))\n",
    "print('\\nRFGB: Test set score (Alice/Persuasion):', clf.score(X_Test1, Y_Test1))\n",
    "print('\\nRFGB: Test set score (Alice/Emma):', clf.score(X_Test2, Y_Test2))\n",
    "\n",
    "print('\\nRFGB: Test set score (Alice/Emma/Persuasion):', clf.score(X_Test3, Y_Test3))\n",
    "print('')\n",
    "Y_Pred = clf.predict(X_Test3)\n",
    "my_confusion_matrix(Y_Test3, Y_Pred, 'Austen')\n",
    "\n",
    "\n",
    "score = cross_val_score(clf, X_Full, Y_Full, cv=5)\n",
    "print(\"\\nRFGBCross Validation (All Books) Accuracy %i folds: %.2f (+/- %.2f)\" % (5, score.mean(), (score.std() * 2)))\n",
    "\n",
    "\n",
    "feature_importance = clf.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "fi = pd.DataFrame(feature_importance, index=X_Train.columns, columns=['Importance'])\n",
    "fi.sort_values(by='Importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n",
    "\n",
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
