{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def my_remove_highly_correlated(X,threshold):\n",
    "\n",
    "    X = X.select_dtypes(include=[np.number])  #.dropna()\n",
    "    \n",
    "    cols = X.columns\n",
    "\n",
    "    #run correlation matrix\n",
    "    df = X.corr()\n",
    "\n",
    "    \n",
    "    #put df into array\n",
    "    a = df.values\n",
    "    #label top half with -99999 \n",
    "    #we want to ignore top half of matrix\n",
    "    iu1 = np.triu_indices(len(df))\n",
    "    a[iu1] = -99999\n",
    "    #put data back into dataframe\n",
    "    df = pd.DataFrame(a, columns=cols)\n",
    "    df['var'] = cols\n",
    "\n",
    "    #unstack to get a list of var1, var2, correlation\n",
    "    df = pd.melt(df, id_vars='var')\n",
    "\n",
    "    #remove those flagged with -99999\n",
    "    df = df[df.value != -99999].sort_values(by='var', ascending=True)\n",
    "\n",
    "    #flag remove vs keep based on corr threshold\n",
    "    df_remove = df[df.value > threshold]\n",
    "    keep_list = df.loc[df.value <= threshold,'var'].unique()\n",
    " \n",
    "    print('{} out of {} vars removed due to corr greater than {}'.\n",
    "          format(df_remove.shape[0],X.shape[1],threshold))\n",
    "    \n",
    "    print(df_remove)\n",
    "    print('\\nShape before: ' + str(X.shape))\n",
    "    print('Shape after: ' + str(X[keep_list].shape))\n",
    "    print('\\n')\n",
    "    return X[keep_list]\n",
    "\n",
    "def my_drop_na_columns(X,NANthreshold):\n",
    "\n",
    "    df = X\n",
    "    colcount = df.shape[1]\n",
    "    #Get count of NA in each column\n",
    "    series_cols = df.isnull().sum(axis = 0).sort_values(ascending=False)\n",
    "\n",
    "    #filter the list to include only those with counts above threshold\n",
    "    series_cols_remove = series_cols[series_cols.values >= NANthreshold]\n",
    "    series_cols_keep = series_cols[(series_cols.values < NANthreshold) & (series_cols.values > 0) ]\n",
    "    \n",
    "    #put the to-remove column names in a list\n",
    "    list_colstodrop = series_cols_remove.index.tolist()\n",
    "\n",
    "    #drop the columns\n",
    "    df = df.drop(labels = list_colstodrop, axis=1)\n",
    "\n",
    "    #print the results\n",
    "    print('Dropped {} of {} Columns - containing more than {} NANs\\n'.format(\n",
    "          series_cols_remove.shape[0],colcount,NANthreshold))\n",
    "    print(series_cols_remove)\n",
    "    \n",
    "    print('\\n{} Columns remain with NANs\\n'.format(series_cols_keep.shape[0]))\n",
    "    print(series_cols_keep)\n",
    "    print('\\nShape before: ' + str(X.shape))\n",
    "    print('Shape after: ' + str(df.shape))\n",
    "    print('\\n')    \n",
    "    return df\n",
    "\n",
    "\n",
    "def my_feature_selector(mytype,X,y=[],threshold=None,k=None):  \n",
    "    \n",
    "    #reduces # of features in given x dataset by one of 3 ways:\n",
    "    #1) selectkbest  2) rfecv (recursive feature elim)  3) variancethreshold\n",
    "\n",
    "    mytype = str.lower(mytype)\n",
    "    \n",
    "    #Retain original column names\n",
    "    orig_cols = X.columns\n",
    "    \n",
    "    #Evaluate type and instatiate object\n",
    "    if mytype == 'selectkbest':\n",
    "        if k == None:\n",
    "            print('Try Again: To run selectKBest you must pass a k for number of features to select')\n",
    "            return X  \n",
    "        if len(y) == 0:\n",
    "            print('Try Again: To run selectKBest you must pass a y vector reflecting outcomes')\n",
    "            return X             \n",
    "        \n",
    "        selector = SelectKBest(chi2, k=k)\n",
    "        df_new = selector.fit_transform(X,y)\n",
    "        keep_list = selector.get_support()\n",
    "        scores = [orig_cols, selector.pvalues_, keep_list]\n",
    "        score_type = 'PValue'\n",
    "        \n",
    "    if mytype == 'rfecv':\n",
    "        if len(y) == 0:\n",
    "            print('Try Again: To run rfecv you must pass a y vector reflecting outcomes')\n",
    "            return X \n",
    "       \n",
    "        estimator = SVR(kernel=\"linear\")\n",
    "        selector = RFECV(estimator, n_jobs=-1)\n",
    "        df_new = selector.fit_transform(X,y)\n",
    "        keep_list = selector.get_support()\n",
    "        scores = [orig_cols, selector.ranking_, keep_list]\n",
    "        score_type = 'Ranking'\n",
    "            \n",
    "    if mytype == 'variancethreshold':\n",
    "        if threshold == None:\n",
    "            print('Try Again: To run VarianceThreshold you must pass a threshold value from 0 to 1. (0 returns all)')\n",
    "            return X     \n",
    "    \n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        df_new = selector.fit_transform(X)\n",
    "        keep_list = selector.get_support()   \n",
    "        scores = [orig_cols, selector.variances_, keep_list]\n",
    "        score_type = 'Variance'\n",
    "        \n",
    "    if (mytype != 'selectkbest') & (mytype != 'rfecv') & (mytype != 'variancethreshold'):  \n",
    "        print('Try Again: Type must be passed as selectkbest, rfecv, or variancethreshold')\n",
    "        return X  \n",
    "    \n",
    "    \n",
    "    #Print Scores\n",
    "    print('---- Running Feature Selection Method: ' + mytype + '-------\\n')\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    scores_df = scores_df.transpose()\n",
    "    scores_df.columns= ['Feature',score_type,'Keep']\n",
    "    print(scores_df.sort_values(by=score_type, ascending=True))\n",
    "\n",
    "    #List features that were removed vs Kept\n",
    "    i = 0\n",
    "    keep = []\n",
    "\n",
    "    for item in keep_list:\n",
    "        col_name = orig_cols[i]\n",
    "        if item==True:\n",
    "            keep.append(col_name)\n",
    "        i = i + 1\n",
    "\n",
    "    #Place resultset of kept features into DataFrame with correct column headers\n",
    "    df_keep = pd.DataFrame(df_new, columns=keep)\n",
    "    \n",
    "    #Print Stats\n",
    "    print('\\nShape before: ' + str(X.shape))\n",
    "    print('Shape after: ' + str(df_keep.shape))\n",
    "    print('\\n')\n",
    "    return df_keep\n",
    "\n",
    "def my_confusion_matrix(array_Expected,array_Predicted,colName):\n",
    "    a = np.array(confusion_matrix(array_Expected, array_Predicted ))\n",
    "    totalExpectedFalse = a[0,0] + a[0,1]\n",
    "    totalExpectedTrue = a[1,0] + a[1,1]\n",
    "    correctFalse = a[0,0] \n",
    "    correctTrue = a[1,1] \n",
    "    correctTruePct = np.round(correctTrue / totalExpectedTrue,3)\n",
    "    correctFalsePct = np.round(correctFalse / totalExpectedFalse,3)\n",
    "    print('Regarding {}, the model correctly predicted {} Negatives out of {} expected Negatives: {}'.format(\n",
    "        colName,correctFalse,totalExpectedFalse,correctFalsePct))\n",
    "    print('Regarding {}, the model correctly predicted {} Positives out of {} expected Positives: {}'.format(\n",
    "        colName,correctTrue,totalExpectedTrue,correctTruePct))    \n",
    "    print(a)\n",
    "\n",
    "def my_minmax_scaler(df, min_val, max_val):\n",
    "    #Take in a dataframe and return a dataframe scaled with min 0, max 1\n",
    "    print('------Scaling Data to Min {}, Max {}------\\n'.format(min_val,max_val))\n",
    "    # Save the column names.\n",
    "    names=df.columns\n",
    "    \n",
    "    #instatiate scaler object\n",
    "    #you can use StandardScaler instead to scale with mean 0 and std 1\n",
    "    scaler = MinMaxScaler(feature_range=(min_val,max_val), copy=True)\n",
    "    \n",
    "    # Scale, then turn the resulting numpy array back into a data frame with the\n",
    "    # correct column names.\n",
    "    scaler.fit(df)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df), columns=names)\n",
    "    print('Scaling Complete')\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>#cakeweek</th>\n",
       "      <th>#wasteless</th>\n",
       "      <th>22-minute meals</th>\n",
       "      <th>3-ingredient recipes</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow squash</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>yonkers</th>\n",
       "      <th>yuca</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>cookbooks</th>\n",
       "      <th>leftovers</th>\n",
       "      <th>snack</th>\n",
       "      <th>snack week</th>\n",
       "      <th>turkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lentil, Apple, and Turkey Wrap</td>\n",
       "      <td>2.500</td>\n",
       "      <td>426.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n",
       "      <td>4.375</td>\n",
       "      <td>403.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potato and Fennel Soup Hodge</td>\n",
       "      <td>3.750</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahi-Mahi in Tomato Olive Sauce</td>\n",
       "      <td>5.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spinach Noodle Casserole</td>\n",
       "      <td>3.125</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  rating  calories  protein  \\\n",
       "0              Lentil, Apple, and Turkey Wrap    2.500     426.0     30.0   \n",
       "1  Boudin Blanc Terrine with Red Onion Confit    4.375     403.0     18.0   \n",
       "2                Potato and Fennel Soup Hodge    3.750     165.0      6.0   \n",
       "3             Mahi-Mahi in Tomato Olive Sauce    5.000       NaN      NaN   \n",
       "4                    Spinach Noodle Casserole    3.125     547.0     20.0   \n",
       "\n",
       "    fat  sodium  #cakeweek  #wasteless  22-minute meals  3-ingredient recipes  \\\n",
       "0   7.0   559.0        0.0         0.0              0.0                   0.0   \n",
       "1  23.0  1439.0        0.0         0.0              0.0                   0.0   \n",
       "2   7.0   165.0        0.0         0.0              0.0                   0.0   \n",
       "3   NaN     NaN        0.0         0.0              0.0                   0.0   \n",
       "4  32.0   452.0        0.0         0.0              0.0                   0.0   \n",
       "\n",
       "    ...    yellow squash  yogurt  yonkers  yuca  zucchini  cookbooks  \\\n",
       "0   ...              0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "1   ...              0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "2   ...              0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "3   ...              0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "4   ...              0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "\n",
       "   leftovers  snack  snack week  turkey  \n",
       "0        0.0    0.0         0.0     1.0  \n",
       "1        0.0    0.0         0.0     0.0  \n",
       "2        0.0    0.0         0.0     0.0  \n",
       "3        0.0    0.0         0.0     0.0  \n",
       "4        0.0    0.0         0.0     0.0  \n",
       "\n",
       "[5 rows x 680 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('epi_r.csv')\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 4 of 680 Columns - containing more than 2000 NANs\n",
      "\n",
      "fat         4183\n",
      "protein     4162\n",
      "sodium      4119\n",
      "calories    4117\n",
      "dtype: int64\n",
      "\n",
      "0 Columns remain with NANs\n",
      "\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Shape before: (20052, 680)\n",
      "Shape after: (20052, 676)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = raw_data\n",
    "df = my_drop_na_columns(df,NANthreshold=2000)\n",
    "#df.dropna(inplace=True)\n",
    "#df[df.isnull().any(axis=1)]\n",
    "\n",
    "#df['fat'] = np.where(df.fat > 100, 17, df.fat)\n",
    "#df['fat'] = np.where(df.fat > 62, 62, df.fat)\n",
    "#df['protein'] = np.where(df.protein > 50, 8, df.protein)\n",
    "#df['protein'] = np.where(df.protein > 28, 28, df.protein)\n",
    "#df['sodium'] = np.where(df.sodium > 1000, 296, df.sodium)\n",
    "#df['calories'] = np.where(df.calories > 2000, 2000, df.calories)\n",
    "df.drop(columns=['bon appétit'], axis=1, inplace=True)\n",
    "\n",
    "Y = np.where(df.rating >= 4, 1.0, 0.0 )\n",
    "X = df.drop(columns=['rating'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 out of 673 vars removed due to corr greater than 0.8\n",
      "                  var     variable     value\n",
      "5569            drink    alcoholic  0.851944\n",
      "133588         london      england  1.000000\n",
      "203584     louisville     kentucky  0.816476\n",
      "208418    pescatarian       kosher  0.871690\n",
      "286510       portland       oregon  0.829149\n",
      "304769       soy free  peanut free  0.940721\n",
      "304819  tree nut free  peanut free  0.829673\n",
      "\n",
      "Shape before: (20052, 673)\n",
      "Shape after: (20052, 672)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = my_remove_highly_correlated(X, threshold=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Running Feature Selection Method: selectkbest-------\n",
      "\n",
      "                Feature       PValue   Keep\n",
      "276      house & garden  1.39663e-38   True\n",
      "185               drink  2.66413e-30   True\n",
      "7             alcoholic  1.06162e-26   True\n",
      "234                 gin  1.42847e-23   True\n",
      "520               roast  1.02085e-20   True\n",
      "618        thanksgiving  5.74861e-20   True\n",
      "132      cocktail party  2.45704e-18   True\n",
      "178              dinner  2.60384e-16   True\n",
      "582              spirit  9.64424e-16   True\n",
      "122           christmas  6.27071e-14   True\n",
      "49              bitters   7.7588e-14   True\n",
      "576            soy free  6.14919e-13   True\n",
      "453         peanut free  1.10902e-12   True\n",
      "250      grill/barbecue  1.81116e-12   True\n",
      "343             low fat  9.91295e-12   True\n",
      "29         backyard bbq  4.33409e-11   True\n",
      "131            cocktail  1.29407e-10   True\n",
      "203                fall   1.2177e-09   True\n",
      "249               grill  2.07034e-09   True\n",
      "595   stuffing/dressing  2.08636e-09   True\n",
      "261       harpercollins  2.25782e-09   True\n",
      "446               pasta  2.63448e-09   True\n",
      "526                 rum  4.38639e-09   True\n",
      "236         goat cheese  7.02565e-09   True\n",
      "626       tree nut free  4.54982e-08   True\n",
      "592            stir-fry   6.7469e-08   True\n",
      "637               vegan  1.01875e-07   True\n",
      "206        father's day  2.75511e-07   True\n",
      "367                meat  4.81619e-07   True\n",
      "621                tofu  1.15198e-06   True\n",
      "..                  ...          ...    ...\n",
      "400         new orleans       0.9198  False\n",
      "279              hummus       0.9198  False\n",
      "461                peru       0.9198  False\n",
      "168             custard       0.9198  False\n",
      "373               miami       0.9198  False\n",
      "264          healdsburg       0.9198  False\n",
      "489        potato salad       0.9198  False\n",
      "378         minneapolis       0.9198  False\n",
      "289              israel       0.9198  False\n",
      "181  dominican republic       0.9198  False\n",
      "339          louisville       0.9198  False\n",
      "282         iced coffee       0.9198  False\n",
      "437              parade     0.922848  False\n",
      "81               butter     0.934086  False\n",
      "466              picnic     0.934871  False\n",
      "567          snack week      0.93597  False\n",
      "350       macadamia nut     0.949216  False\n",
      "274           hot drink     0.951055  False\n",
      "64            breakfast     0.956941  False\n",
      "628      tropical fruit     0.959083  False\n",
      "503   quick and healthy     0.961915  False\n",
      "552              sesame     0.963018  False\n",
      "154            couscous     0.967198  False\n",
      "511             ramadan     0.970495  False\n",
      "260            hanukkah     0.972037  False\n",
      "411              nutmeg     0.974578  False\n",
      "79               bulgur     0.980949  False\n",
      "105            chambord     0.986528  False\n",
      "191                 egg     0.991538  False\n",
      "187                duck     0.997546  False\n",
      "\n",
      "[672 rows x 3 columns]\n",
      "\n",
      "Shape before: (20052, 672)\n",
      "Shape after: (20052, 30)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = my_feature_selector(mytype='selectkbest', X=X, y=Y, k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Scaling Data to Min 0, Max 1------\n",
      "\n",
      "Scaling Complete\n"
     ]
    }
   ],
   "source": [
    "X = my_minmax_scaler(X, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5799920207460603\n"
     ]
    }
   ],
   "source": [
    "#'----------  SVC ---------------\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "svc = LinearSVC(C=30)\n",
    "svc.fit(X,Y)\n",
    "print(svc.score(X, Y))\n",
    "\n",
    "svm_pred = svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5831837223219629\n"
     ]
    }
   ],
   "source": [
    "#'----------- K Nearest Neighbor --------------\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbors = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "neighbors.fit(X,Y)\n",
    "print(neighbors.score(X, Y))\n",
    "neighbors_pred = neighbors.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5662776780371035\n"
     ]
    }
   ],
   "source": [
    "#'----------- Naive Bayes --------------\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, Y)\n",
    "print(bnb.score(X, Y))\n",
    "bnb_pred = bnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5779473369239976\n"
     ]
    }
   ],
   "source": [
    "#'----------- Logistic Regression --------------\n",
    "from sklearn import linear_model \n",
    "lr = linear_model.LogisticRegression(penalty='l1', C=1 )\n",
    "lr.fit(X, Y)\n",
    "print(lr.score(X, Y))\n",
    "lr_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- svm -------\n",
      "\n",
      "Regarding highly_rated, the model correctly predicted 2608 Negatives out of 9314 expected Negatives: 0.28\n",
      "Regarding highly_rated, the model correctly predicted 9022 Positives out of 10738 expected Positives: 0.84\n",
      "[[2608 6706]\n",
      " [1716 9022]]\n",
      "\n",
      "---- K Nearest Neighbor -------\n",
      "\n",
      "Regarding highly_rated, the model correctly predicted 4457 Negatives out of 9314 expected Negatives: 0.479\n",
      "Regarding highly_rated, the model correctly predicted 7237 Positives out of 10738 expected Positives: 0.674\n",
      "[[4457 4857]\n",
      " [3501 7237]]\n",
      "\n",
      "---- Naive Bayes -------\n",
      "\n",
      "Regarding highly_rated, the model correctly predicted 4691 Negatives out of 9314 expected Negatives: 0.504\n",
      "Regarding highly_rated, the model correctly predicted 6664 Positives out of 10738 expected Positives: 0.621\n",
      "[[4691 4623]\n",
      " [4074 6664]]\n",
      "\n",
      "---- Logistic Regression -------\n",
      "\n",
      "Regarding highly_rated, the model correctly predicted 2685 Negatives out of 9314 expected Negatives: 0.288\n",
      "Regarding highly_rated, the model correctly predicted 8904 Positives out of 10738 expected Positives: 0.829\n",
      "[[2685 6629]\n",
      " [1834 8904]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n---- svm -------\\n')\n",
    "my_confusion_matrix(Y, svm_pred, 'highly_rated')\n",
    "\n",
    "\n",
    "print('\\n---- K Nearest Neighbor -------\\n')\n",
    "my_confusion_matrix(Y, neighbors_pred, 'highly_rated')\n",
    "\n",
    "print('\\n---- Naive Bayes -------\\n')\n",
    "my_confusion_matrix(Y, bnb_pred, 'highly_rated')\n",
    "\n",
    "print('\\n---- Logistic Regression -------\\n')\n",
    "my_confusion_matrix(Y, lr_pred, 'highly_rated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- svm -------\n",
      "\n",
      "Cross Validation Accuracy 5 folds: 0.58 (+/- 0.01)\n",
      "\n",
      "---- K Nearest Neighbor -------\n",
      "\n",
      "Cross Validation Accuracy 5 folds: 0.55 (+/- 0.02)\n",
      "\n",
      "---- Naive Bayes -------\n",
      "\n",
      "Cross Validation Accuracy 5 folds: 0.57 (+/- 0.02)\n",
      "\n",
      "---- Logistic Regression -------\n",
      "\n",
      "Cross Validation Accuracy 5 folds: 0.58 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "cv=5\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('\\n---- svm -------')\n",
    "score = cross_val_score(svc, X, Y, cv=cv)\n",
    "print(\"\\nCross Validation Accuracy %i folds: %.2f (+/- %.2f)\" % (cv, score.mean(), (score.std() * 2)))\n",
    "\n",
    "print('\\n---- K Nearest Neighbor -------')\n",
    "score = cross_val_score(neighbors, X, Y, cv=cv)\n",
    "print(\"\\nCross Validation Accuracy %i folds: %.2f (+/- %.2f)\" % (cv, score.mean(), (score.std() * 2)))\n",
    "\n",
    "\n",
    "print('\\n---- Naive Bayes -------')\n",
    "score = cross_val_score(bnb, X, Y, cv=cv)\n",
    "print(\"\\nCross Validation Accuracy %i folds: %.2f (+/- %.2f)\" % (cv, score.mean(), (score.std() * 2)))\n",
    "\n",
    "\n",
    "\n",
    "print('\\n---- Logistic Regression -------')\n",
    "score = cross_val_score(lr, X, Y, cv=cv)\n",
    "print(\"\\nCross Validation Accuracy %i folds: %.2f (+/- %.2f)\" % (cv, score.mean(), (score.std() * 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Oh dear, so this did seem not to work very well. In fact it is remarkably poor. Now there are many things that we could do here. \n",
    "\n",
    "Firstly the overfit is a problem, even though it was poor in the first place. We could go back and clean up our feature set. There might be some gains to be made by getting rid of the noise.\n",
    "\n",
    "We could also see how removing the nulls but including dietary information performs. Though its a slight change to the question we could still possibly get some improvements there.\n",
    "\n",
    "Lastly, we could take our regression problem and turn it into a classifier. With this number of features and a discontinuous outcome, we might have better luck thinking of this as a classification problem. We could make it simpler still by instead of classifying on each possible value, group reviews to some decided high and low values.\n",
    "\n",
    "__And that is your challenge.__\n",
    "\n",
    "Transform this regression problem into a binary classifier and clean up the feature set. You can choose whether or not to include nutritional information, but try to cut your feature set down to the 30 most valuable features.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low</th>\n",
       "      <th>High</th>\n",
       "      <th>All</th>\n",
       "      <th>PercentBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stuffing/dressing</th>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>226.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goat cheese</th>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>102.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father's day</th>\n",
       "      <td>0.009985</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>88.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meat</th>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>87.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roast</th>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.065829</td>\n",
       "      <td>71.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grill</th>\n",
       "      <td>0.020507</td>\n",
       "      <td>0.034736</td>\n",
       "      <td>0.028127</td>\n",
       "      <td>69.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thanksgiving</th>\n",
       "      <td>0.054005</td>\n",
       "      <td>0.088936</td>\n",
       "      <td>0.072711</td>\n",
       "      <td>64.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christmas</th>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.051815</td>\n",
       "      <td>62.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backyard bbq</th>\n",
       "      <td>0.035967</td>\n",
       "      <td>0.056156</td>\n",
       "      <td>0.046778</td>\n",
       "      <td>56.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grill/barbecue</th>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>55.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dinner</th>\n",
       "      <td>0.112089</td>\n",
       "      <td>0.154684</td>\n",
       "      <td>0.134899</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fall</th>\n",
       "      <td>0.132489</td>\n",
       "      <td>0.165860</td>\n",
       "      <td>0.150359</td>\n",
       "      <td>25.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soy free</th>\n",
       "      <td>0.368692</td>\n",
       "      <td>0.433414</td>\n",
       "      <td>0.403351</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peanut free</th>\n",
       "      <td>0.383509</td>\n",
       "      <td>0.448687</td>\n",
       "      <td>0.418412</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree nut free</th>\n",
       "      <td>0.326712</td>\n",
       "      <td>0.372602</td>\n",
       "      <td>0.351287</td>\n",
       "      <td>14.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vegan</th>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>0.092310</td>\n",
       "      <td>-21.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta</th>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.039672</td>\n",
       "      <td>0.048274</td>\n",
       "      <td>-31.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low fat</th>\n",
       "      <td>0.057440</td>\n",
       "      <td>0.036692</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>-36.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cocktail party</th>\n",
       "      <td>0.073438</td>\n",
       "      <td>0.043770</td>\n",
       "      <td>0.057550</td>\n",
       "      <td>-40.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rum</th>\n",
       "      <td>0.022225</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>0.016507</td>\n",
       "      <td>-48.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cocktail</th>\n",
       "      <td>0.025660</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>-48.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink</th>\n",
       "      <td>0.074404</td>\n",
       "      <td>0.036692</td>\n",
       "      <td>0.054209</td>\n",
       "      <td>-50.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stir-fry</th>\n",
       "      <td>0.015675</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>-51.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcoholic</th>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.041642</td>\n",
       "      <td>-53.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tofu</th>\n",
       "      <td>0.007086</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>-65.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spirit</th>\n",
       "      <td>0.017286</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>-68.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harpercollins</th>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>-68.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house &amp; garden</th>\n",
       "      <td>0.037793</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.022890</td>\n",
       "      <td>-73.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitters</th>\n",
       "      <td>0.010844</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>-77.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gin</th>\n",
       "      <td>0.019326</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>-77.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Low      High       All  PercentBoost\n",
       "stuffing/dressing  0.003114  0.010151  0.006882        226.02\n",
       "goat cheese        0.009878  0.020022  0.015310        102.70\n",
       "father's day       0.009985  0.018812  0.014712         88.40\n",
       "meat               0.009663  0.018160  0.014213         87.93\n",
       "roast              0.047670  0.081579  0.065829         71.13\n",
       "grill              0.020507  0.034736  0.028127         69.39\n",
       "thanksgiving       0.054005  0.088936  0.072711         64.68\n",
       "christmas          0.038866  0.063047  0.051815         62.22\n",
       "backyard bbq       0.035967  0.056156  0.046778         56.13\n",
       "grill/barbecue     0.042517  0.065934  0.055057         55.08\n",
       "dinner             0.112089  0.154684  0.134899         38.00\n",
       "fall               0.132489  0.165860  0.150359         25.19\n",
       "soy free           0.368692  0.433414  0.403351         17.55\n",
       "peanut free        0.383509  0.448687  0.418412         17.00\n",
       "tree nut free      0.326712  0.372602  0.351287         14.05\n",
       "vegan              0.104574  0.081673  0.092310        -21.90\n",
       "pasta              0.058192  0.039672  0.048274        -31.83\n",
       "low fat            0.057440  0.036692  0.046330        -36.12\n",
       "cocktail party     0.073438  0.043770  0.057550        -40.40\n",
       "rum                0.022225  0.011548  0.016507        -48.04\n",
       "cocktail           0.025660  0.013131  0.018951        -48.83\n",
       "drink              0.074404  0.036692  0.054209        -50.69\n",
       "stir-fry           0.015675  0.007543  0.011321        -51.88\n",
       "alcoholic          0.058192  0.027286  0.041642        -53.11\n",
       "tofu               0.007086  0.002421  0.004588        -65.83\n",
       "spirit             0.017286  0.005401  0.010922        -68.75\n",
       "harpercollins      0.009556  0.002980  0.006034        -68.81\n",
       "house & garden     0.037793  0.009965  0.022890        -73.63\n",
       "bitters            0.010844  0.002421  0.006334        -77.67\n",
       "gin                0.019326  0.004284  0.011271        -77.83"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = X\n",
    "review['rating'] = Y\n",
    "\n",
    "table = review.pivot_table(review, index=['rating'], aggfunc=np.average, margins=True, )\n",
    "\n",
    "\n",
    "table = table.transpose().sort_values(by='All', ascending=False)\n",
    "\n",
    "\n",
    "table.columns = ['Low','High','All']\n",
    "table['PercentBoost'] = round((table['High'] - table['Low']) / table.Low * 100,2)\n",
    "table.sort_values(by='PercentBoost', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "When you've finished that, also take a moment to think about bias. Is there anything in this dataset that makes you think it could be biased, perhaps extremely so?\n",
    "\n",
    "There is. Several things in fact, but most glaringly is that we don't actually have a random sample. It could be, and probably is, that the people more likely to choose some kinds of recipes are more likely to give high reviews.\n",
    "\n",
    "After all, people who eat chocolate _might_ just be happier people."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
